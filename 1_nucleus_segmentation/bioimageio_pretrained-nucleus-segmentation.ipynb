{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0a3e1c0",
   "metadata": {},
   "source": [
    "## Nucleus Segmentation with a pre-trained network\n",
    "\n",
    "In this exercise we will segment the nuclei in the image data. This segmentation will be used further to segment the complete cells (we will explain this in one of the next exercises). Here, we use a model from [bioimage.io](https://bioimage.io/#/) for nucleus segmentation. It was trained on data from the [DSB Nucleus Segmentation](https://www.kaggle.com/c/data-science-bowl-2018), which contains images quite similar to the nucleus channel here. Hence, it works quite well without changing the model at all.\n",
    "\n",
    "In this exercise we will learn how to apply a pre-trained model from [bioimage.io](https://bioimage.io/#/) with the [bioimageio.core library](https://github.com/bioimage-io/core-bioimage-io-python). More generally, we will see the value of pre-trained models that can often yield quite good results for our image analysis tasks if they have been trained on similar data. Bioimage.io is a resource that hosts pre-trained deep learning models for microscopy image analysis tasks. \n",
    "\n",
    "Note: there are several other deep learning based tools for nucleus segmentation. In particular, [stardist](https://github.com/stardist/stardist) is a versatile and robust choice for this task. The notebook `stardist_pretrained-nucleus-segmentation` demonstrates how to use it for nucleus segmentation on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general purpose libraries.\n",
    "import os\n",
    "\n",
    "import imageio.v3 as imageio\n",
    "import napari\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the folders with data and output.\n",
    "# You should have downloaded the data already in the data visualization notebook.\n",
    "# if you store the data somewhere else just change the 'data_folder' variable.\n",
    "data_folder = \"../data\"\n",
    "output_folder = os.path.join(data_folder, \"predictions\")\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abbb6a76",
   "metadata": {},
   "source": [
    "### 1. Load the nucleus segmentation model\n",
    "\n",
    "We use the following model from bioimage.io: [affable-shark](https://bioimage.io/#/?tags=nuclei&id=10.5281%2Fzenodo.5764892). Each model has an animal nickname; the one we will use predicts foreground and boundary probabilities for microscopy images with nucleus staining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for prediction with bioimageio models.\n",
    "import bioimageio.core\n",
    "from xarray import DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first download the model. 'load_resource_description' will do this and will also create an object ('model')\n",
    "# that we can use to apply the model.\n",
    "model_name = \"affable-shark\"  # nickname of the model\n",
    "model = bioimageio.core.load_resource_description(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7608fbb2",
   "metadata": {},
   "source": [
    "### 2. Check the model\n",
    "\n",
    "Now we run prediction for one image with this model and visualize the predictions with napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3406ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images with nucleus staining for one of the test images\n",
    "image_path = os.path.join(data_folder, \"test/gt_image_040/gt_image_040_nucleus_image.tif\")\n",
    "image = imageio.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction with the model. We use the 'PredictionPipeline' from bioimage.io for this.\n",
    "# It applies preprocessing (typically image normalization), then applies the model itself to the data\n",
    "# and optionally also post-process the data.\n",
    "# All these steps are specified in the model description we have just loaded.\n",
    "with bioimageio.core.create_prediction_pipeline(model) as prediction_pipeline:\n",
    "    # The input to the model needs to have four dimensions:\n",
    "    # A batch and channel axis (these both have the values one in our case) and the two image axes.\n",
    "    # The code here adds the batch and channel axis and then adds names to the axes. This is required by the bioimage.io library.\n",
    "    input_ = DataArray(image[None, None], dims=tuple(\"bcyx\"))\n",
    "    # Run prediction.\n",
    "    prediction = prediction_pipeline(input_)[0].squeeze().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction in napari.\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image)\n",
    "viewer.add_image(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1b5e5-caeb-4d6e-8786-c68510dc0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: @Antonio, Luca: I have only updated the notebook up until here.\n",
    "# We can see if we want to simplify the post-processing that follows and if we update or extend the exercises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db41f2c2",
   "metadata": {},
   "source": [
    "### 3. Implement post-processing to get the nucleus instance segmentation\n",
    "\n",
    "As you have seen the model predicts foreground and nucleus boundaries. However, we want to segment individual nuclei, i.e. an \"image\" where each nucleus is marked by an unique id. So we post-process the network predictions to get the instance segmentation:\n",
    "- compute the distance map to the boundary predictions\n",
    "- set it to zero outside of the predicted foreground\n",
    "- find the distance maxima\n",
    "- run seeded watershed from these maxima using the boundary predictions as height map\n",
    "\n",
    "We choose this approach in order to separate touching nuclei with weak boundary evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90642bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the functions from scipy and skimage we need to implement the instance segmentation procedure\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.filters import gaussian\n",
    "from skimage.segmentation import watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distances to nearest boundaries inside the predicted foreground\n",
    "foreground, boundaries = prediction\n",
    "foreground = foreground > 0.5\n",
    "boundary_distances = distance_transform_edt(boundaries < 0.1)\n",
    "boundary_distances[~foreground] = 0\n",
    "boundary_distances = gaussian(boundary_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b05ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the seeds (= maxima of the distance map) and run seeded watershed\n",
    "seed_points = peak_local_max(boundary_distances, min_distance=5, exclude_border=False)\n",
    "seeds = np.zeros(foreground.shape, dtype=\"uint32\")\n",
    "seeds[seed_points[:, 0], seed_points[:, 1]] = np.arange(1, len(seed_points) + 1)\n",
    "nucleus_segmentation = watershed(boundaries, markers=seeds, mask=foreground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1668429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the segmentation result and visualize the intermediates\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image)\n",
    "viewer.add_image(boundaries)\n",
    "viewer.add_image(boundary_distances)\n",
    "viewer.add_points(seed_points)\n",
    "viewer.add_labels(nucleus_segmentation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b15e287",
   "metadata": {},
   "source": [
    "### 4. Apply to all test data\n",
    "\n",
    "After checking the predictions and segmentation procedure for one image we apply it to all test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4774ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd329f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the test files\n",
    "input_files = glob(os.path.join(data_folder, \"test\", \"*.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what shapes we have in the images\n",
    "shapes = [np.array(h5py.File(path, \"r\")[\"raw/nuclei/s0\"].shape) for path in input_files]\n",
    "print(\"Image shapes:\", np.unique(shapes, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62155846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape that is expected by the model\n",
    "print(model.inputs[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a64e2ff",
   "metadata": {},
   "source": [
    "When you run above code you will see that we have two different image shapes: `(930, 1024)`, `(1024, 1024)`. And you will see that the model has the following input shape description: `ParametrizedInputShape(min=[1, 1, 64, 64], step=[0, 0, 16, 16])`. This description means that it expects inputs of a minimal shape of `(64, 64)` and that the input shape needs to be divisible by `16`. Hence, the image height `930` is not a valid input to the model. That's why we use the function `predict_with_padding` below, which automatically pads the input image to the expected input shape, runs prediction and then crops the prediction results back to the input shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ea34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that wraps the prediction and segmentation functionality we have tested above\n",
    "def run_segmentation(pp, image, min_distance=5, sigma=1.0):\n",
    "    input_ = DataArray(image[None, None], dims=tuple(\"bcyx\"))\n",
    "    # prediction with padding to deal with images with incompatible input shape\n",
    "    prediction = bioimageio.core.predict_with_padding(pp, input_, padding={\"x\": 16, \"y\": 16})[0].squeeze().values\n",
    "    foreground, boundaries = prediction\n",
    "    foreground = foreground > 0.5\n",
    "    boundary_distances = distance_transform_edt(boundaries < 0.1)\n",
    "    boundary_distances[~foreground] = 0\n",
    "    boundary_distances = gaussian(boundary_distances, sigma)\n",
    "    seed_points = peak_local_max(boundary_distances, min_distance=min_distance, exclude_border=False)\n",
    "    seeds = np.zeros(foreground.shape, dtype=\"uint32\")\n",
    "    seeds[seed_points[:, 0], seed_points[:, 1]] = np.arange(1, len(seed_points) + 1)\n",
    "    nucleus_segmentation = watershed(boundaries, markers=seeds, mask=foreground)\n",
    "    return nucleus_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run segmentation for all test images\n",
    "with bioimageio.core.create_prediction_pipeline(model) as pp:\n",
    "    for path in tqdm(input_files, desc=\"Run nucleus segmentation\"):\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            image = f[\"raw/nuclei/s0\"][:]\n",
    "        segmentation = run_segmentation(pp, image)\n",
    "        output_path = os.path.join(output_folder, os.path.basename(path))\n",
    "        with h5py.File(output_path, \"a\") as f:\n",
    "            f.create_dataset(\"segmentations/nuclei/watershed_based\", data=segmentation, compression=\"gzip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94f7db4b",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- We have segmented the nuclei, but haven't evaluated the quality of the instance segmentation yet. For example, we can evaluate its quality by computing the average precision at 50% overlap (AP50) when comparing the ground-truth nucleus segmentation to our segmentation results. Implement the evaluation procedure:\n",
    "    - First, compute the ground-truth nucleus instance segmentation by applying connected components ([skimage.measure.label](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.label)) to the semantic nucleus segmentation ground-truth, which is stored in the key `labels/infected/nuclei/s0`.\n",
    "    - Then compute the AP50 score using the function [elf.evaluation.matching](https://github.com/constantinpape/elf/blob/master/elf/evaluation/matching.py#L129).\n",
    "    - Hint: you can check out a similar evaluation procedure in the notebook `2_cell_segmentation/cell_segmentation.ipynb`.\n",
    "- [stardist](https://github.com/stardist/stardist) is another pre-trained method that can be used to segment nuclei, and is generally more robust than the approach used here. Segment the nuclei from the test set with it, run evaluation and compare the results with our segmentations from here.\n",
    "    - We are also working on an example notebook for showing how to apply segmentation with stardist to this data in `stardist_pretrained-nucleus-segmentation.ipynb` but it is not finished yet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34c9e45d",
   "metadata": {},
   "source": [
    "### What's next\n",
    "\n",
    "Next we turn to segmenting the full cells (i.e. cytosol and nucleus). For this, we will first train a network for predicting cell foreground and boundaries in `2_cell_segmentation/torchem-train-cell-membrane-segmentation.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
