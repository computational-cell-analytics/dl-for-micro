{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0a3e1c0",
   "metadata": {},
   "source": [
    "## Nucleus Segmentation with a pre-trained network\n",
    "\n",
    "As the first step we will segment the nuclei in the input images. The nucleus segmentation will be used to segment the full cells by using a seeded watershed starting from the nuclei (will be discussed in more detail in one of the next lessons). Here, we will use a model from [bioimage.io](https://bioimage.io/#/) for nucleus segmentation. This model was trained on data from the [DSB Nucleus Segmentation](https://www.kaggle.com/c/data-science-bowl-2018), which contains images quite similar to the nucleus channel here. Hence, it works quite well without changing the model at all.\n",
    "\n",
    "The goal of this session is to learn how to apply a pre-trained model from [bioimage.io](https://bioimage.io/#/) using the [bioimageio.core](https://github.com/bioimage-io/core-bioimage-io-python) python library.\n",
    "\n",
    "Note: there are several other deep learning based tools for nucleus segmentation. In particular, [stardist](https://github.com/stardist/stardist) is a versatile and robust choice for this task. The notebook `stardist_pretrained-nucleus-segmentation` demonstrates how to use it for this task instead of the model here (work in progress!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general purpose libraries\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import napari\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the paths to folders with data and output.\n",
    "# if you store the data somewhere else just change the 'data_folder' variable.\n",
    "data_folder = \"../data\"\n",
    "output_folder = os.path.join(data_folder, \"predictions\")\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abbb6a76",
   "metadata": {},
   "source": [
    "### 1. Load the nucleus segmentation model\n",
    "\n",
    "We use this model from bioimage.io: https://bioimage.io/#/?tags=nuclei&id=10.5281%2Fzenodo.5764892.\n",
    "It is a U-Net that was trained to predict foreground and boundaries in microscopy images with nucleus staining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for bioimageio\n",
    "import bioimageio.core\n",
    "from xarray import DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affable-shark is the nickname of the model we want to use.\n",
    "# load_resource_description downloads this model and loads it into memory\n",
    "# in the representation of the bioimageio.core library\n",
    "model_name = \"affable-shark\"\n",
    "model = bioimageio.core.load_resource_description(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7608fbb2",
   "metadata": {},
   "source": [
    "### 2. Check the model\n",
    "\n",
    "Next we run prediction for one image with this model and visualize the prediction with napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3406ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the channel with nucleus staining for one of the test images (from hdf5)\n",
    "image_path = os.path.join(data_folder, \"test/gt_image_040.h5\")\n",
    "with h5py.File(image_path, \"r\") as f:\n",
    "    image = f[\"raw/nuclei/s0\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction with the bioimageio.core library using the prediction pipeline class,\n",
    "# which applies the pre-and-postprocessing defined in the bioimageio model specification\n",
    "# as well as the deep learning model (here: U-Net) itself\n",
    "with bioimageio.core.create_prediction_pipeline(model) as pp:\n",
    "    input_ = DataArray(image[None, None], dims=tuple(\"bcyx\"))\n",
    "    prediction = pp(input_)[0].squeeze().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the prediction in napari\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image)\n",
    "viewer.add_image(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db41f2c2",
   "metadata": {},
   "source": [
    "### 3. Implement post-processing to get the nucleus instance segmentation\n",
    "\n",
    "As you have seen the model predicts foreground and nucleus boundaries. However, we want to segment individual nuclei, i.e. an \"image\" where each nucleus is marked by an unique id. So we post-process the network predictions to get the instance segmentation:\n",
    "- compute the distance map to the boundary predictions\n",
    "- set it to zero outside of the predicted foreground\n",
    "- find the distance maxima\n",
    "- run seeded watershed from these maxima using the boundary predictions as height map\n",
    "\n",
    "We choose this approach in order to separate touching nuclei with weak boundary evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90642bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the functions from scipy and skimage we need to implement the instance segmentation procedure\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.filters import gaussian\n",
    "from skimage.segmentation import watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distances to nearest boundaries inside the predicted foreground\n",
    "foreground, boundaries = prediction\n",
    "foreground = foreground > 0.5\n",
    "boundary_distances = distance_transform_edt(boundaries < 0.1)\n",
    "boundary_distances[~foreground] = 0\n",
    "boundary_distances = gaussian(boundary_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b05ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the seeds (= maxima of the distance map) and run seeded watershed\n",
    "seed_points = peak_local_max(boundary_distances, min_distance=5, exclude_border=False)\n",
    "seeds = np.zeros(foreground.shape, dtype=\"uint32\")\n",
    "seeds[seed_points[:, 0], seed_points[:, 1]] = np.arange(1, len(seed_points) + 1)\n",
    "nucleus_segmentation = watershed(boundaries, markers=seeds, mask=foreground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1668429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the segmentation result and visualize the intermediates\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image)\n",
    "viewer.add_image(boundaries)\n",
    "viewer.add_image(boundary_distances)\n",
    "viewer.add_points(seed_points)\n",
    "viewer.add_labels(nucleus_segmentation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b15e287",
   "metadata": {},
   "source": [
    "### 4. Apply to all test data\n",
    "\n",
    "After checking the predictions and segmentation procedure for one image we apply it to all test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4774ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd329f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the test files\n",
    "input_files = glob(os.path.join(data_folder, \"test\", \"*.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what shapes we have in the images\n",
    "shapes = [np.array(h5py.File(path, \"r\")[\"raw/nuclei/s0\"].shape) for path in input_files]\n",
    "print(\"Image shapes:\", np.unique(shapes, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62155846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape that is expected by the model\n",
    "print(model.inputs[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a64e2ff",
   "metadata": {},
   "source": [
    "When you run above code you will see that we have two different image shapes: `(930, 1024)`, `(1024, 1024)`. And you will see that the model has the following input shape description: `ParametrizedInputShape(min=[1, 1, 64, 64], step=[0, 0, 16, 16])`. This description means that it expects inputs of a minimal shape of `(64, 64)` and that the input shape needs to be divisible by `16`. Hence, the image height `930` is not a valid input to the model. That's why we use the function `predict_with_padding` below, which automatically pads the input image to the expected input shape, runs prediction and then crops the prediction results back to the input shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ea34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that wraps the prediction and segmentation functionality we have tested above\n",
    "def run_segmentation(pp, image, min_distance=5, sigma=1.0):\n",
    "    input_ = DataArray(image[None, None], dims=tuple(\"bcyx\"))\n",
    "    # prediction with padding to deal with images with incompatible input shape\n",
    "    prediction = bioimageio.core.predict_with_padding(pp, input_, padding={\"x\": 16, \"y\": 16})[0].squeeze().values\n",
    "    foreground, boundaries = prediction\n",
    "    foreground = foreground > 0.5\n",
    "    boundary_distances = distance_transform_edt(boundaries < 0.1)\n",
    "    boundary_distances[~foreground] = 0\n",
    "    boundary_distances = gaussian(boundary_distances, sigma)\n",
    "    seed_points = peak_local_max(boundary_distances, min_distance=min_distance, exclude_border=False)\n",
    "    seeds = np.zeros(foreground.shape, dtype=\"uint32\")\n",
    "    seeds[seed_points[:, 0], seed_points[:, 1]] = np.arange(1, len(seed_points) + 1)\n",
    "    nucleus_segmentation = watershed(boundaries, markers=seeds, mask=foreground)\n",
    "    return nucleus_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run segmentation for all test images\n",
    "with bioimageio.core.create_prediction_pipeline(model) as pp:\n",
    "    for path in tqdm(input_files, desc=\"Run nucleus segmentation\"):\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            image = f[\"raw/nuclei/s0\"][:]\n",
    "        segmentation = run_segmentation(pp, image)\n",
    "        output_path = os.path.join(output_folder, os.path.basename(path))\n",
    "        with h5py.File(output_path, \"a\") as f:\n",
    "            f.create_dataset(\"segmentations/nuclei/watershed_based\", data=segmentation, compression=\"gzip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94f7db4b",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- We have segmented the nuclei, but haven't evaluated the quality of the instance segmentation yet. For example, we can evaluate its quality by computing the average precision at 50% overlap (AP50) when comparing the ground-truth nucleus segmentation to our segmentation results. Implement the evaluation procedure:\n",
    "    - First, compute the ground-truth nucleus instance segmentation by applying connected components ([skimage.measure.label](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.label)) to the semantic nucleus segmentation ground-truth, which is stored in the key `labels/infected/nuclei/s0`.\n",
    "    - Then compute the AP50 score using the function [elf.evaluation.matching](https://github.com/constantinpape/elf/blob/master/elf/evaluation/matching.py#L129).\n",
    "    - Hint: you can check out a similar evaluation procedure in the notebook `2_cell_segmentation/cell_segmentation.ipynb`.\n",
    "- [stardist](https://github.com/stardist/stardist) is another pre-trained method that can be used to segment nuclei, and is generally more robust than the approach used here. Segment the nuclei from the test set with it, run evaluation and compare the results with our segmentations from here.\n",
    "    - We are also working on an example notebook for showing how to apply segmentation with stardist to this data in `stardist_pretrained-nucleus-segmentation.ipynb` but it is not finished yet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34c9e45d",
   "metadata": {},
   "source": [
    "### What's next\n",
    "\n",
    "Next we turn to segmenting the full cells (i.e. cytosol and nucleus). For this, we will first train a network for predicting cell foreground and boundaries in `2_cell_segmentation/torchem-train-cell-membrane-segmentation.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
