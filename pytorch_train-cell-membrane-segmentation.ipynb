{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878447fb-1a08-4efc-93b1-1895e50dcfcb",
   "metadata": {},
   "source": [
    "# Train a U-Net for Cell Segmentation\n",
    "To segment the full cell (cytosol + nucleus) we first train a U-Net to predict the foreground and boundaries of the cells. We will later uses these predictions to segment the cells using a seeded watershed from the nucleus predictions (see previous notebooks).\n",
    "\n",
    "Here, we will use the pytorch library to train a U-Net using the images and ground-truth data we have downloaded. The goal of this lesson is learning how to use this library for training segmentation models.\n",
    "\n",
    "Note: we are also working on a notebook that shows how to train a U-Net using only PyTorch in pytorch_train-cell-membrane-segmentation.ipynb. (This is not yet finished.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed9f09-e439-4914-b8de-27d4ca9dcc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports.\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import imageio.v3 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import napari\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage.metrics import contingency_table, peak_signal_noise_ratio\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc188708-d8e7-4e7a-ba68-e749a973477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will download and unpack the data and do some further data preparation.\n",
    "# It will only be executed if the data has not been downloaded yet.\n",
    "data_dir = \"../data\"\n",
    "if os.path.exists(data_dir):\n",
    "    print(\"The data is downloaded already.\")\n",
    "else:\n",
    "    utils.prepare_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246a3c5-8f9d-4f27-b7d7-e8c039f8f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model = None\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7154af-58da-418d-9397-de23f7a346ff",
   "metadata": {},
   "source": [
    "### 1. Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51ed2b-8bcc-4dd2-a68e-11c88b66b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data has been downloaded and separated into folders for the train, validation and test split already.\n",
    "# We first create a dictionary with the location of the three different split folders.\n",
    "data_dirs = {\n",
    "    \"train\": os.path.join(data_dir, \"train\"),\n",
    "    \"val\": os.path.join(data_dir, \"val\"),\n",
    "    \"test\": os.path.join(data_dir, \"test\")\n",
    "}\n",
    "\n",
    "# And check the content for one of the samples.\n",
    "# After the print you should see the images (marker, nuclei, serum), labels (cells and nuclei) as well as a json file.\n",
    "train_sample0 = os.path.join(data_dirs[\"train\"], \"gt_image_000\")\n",
    "print(os.listdir(train_sample0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d5f45-9594-4e21-84b1-b29d8636732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the function that cuts out the small images centered around the cells for one image.\n",
    "\n",
    "# This is a helper function for normalizing an image to the range [0, 1].\n",
    "# Data normalization is important when training neural networks for image data\n",
    "# to make sure all the inputs are in the same data range.\n",
    "def normalize(image):\n",
    "    image = image.astype(\"float32\")\n",
    "    #image = skimage.img_as_float(image)\n",
    "    image -= image.min()\n",
    "    image /= (image.max() + 1e-7)\n",
    "    return image\n",
    "\n",
    "\n",
    "# This is the main function for extracting the small images.\n",
    "# We give it the path to the folder containing the data for a sample as input.\n",
    "# Remember that this contains the images, segmentations and classification data\n",
    "# in individual files for each sample. We have explored some of this data above.\n",
    "def extract_images_and_labels(sample_folder):\n",
    "\n",
    "    # We create the filepaths for the image and segmentation data we will load. \n",
    "    sample_name = os.path.basename(sample_folder)  # We can derive the sample name from the foldername.\n",
    "\n",
    "    # serum is the image and cell is the label\n",
    "    serum_path = os.path.join(sample_folder, f\"{sample_name}_serum_image.tif\")\n",
    "    cell_segmentation_path = os.path.join(sample_folder, f\"{sample_name}_cell_labels.tif\")\n",
    "\n",
    "    # And load the serum image and the cell segmentation.\n",
    "    # We normalize the serum image so that the data range is in [0, 1].\n",
    "    serum = normalize(imageio.imread(serum_path))\n",
    "    # Note that we must not normalize the segmentation!\n",
    "    cells = imageio.imread(cell_segmentation_path)\n",
    "    # convert into binary mask\n",
    "    # mask = (cells != 0)\n",
    "    # mask[mask] = 1\n",
    "\n",
    "    return serum, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf88e40-c0fe-4cd5-b968-cebe0ab902b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies the data extraction function we just defined to all samples\n",
    "# for a split (training, validation or test) and extracts all the corresponding small images\n",
    "# and classification labels.\n",
    "def prepare_split(split):\n",
    "    # Get all the folders for the samples of this split.\n",
    "    split_folder = data_dirs[split]\n",
    "    samples = glob(os.path.join(split_folder, \"gt*\"))\n",
    "    # Iterate over all the samples and extract the images and labels from them.\n",
    "    images, labels = [], []\n",
    "    for sample in samples:\n",
    "        sample_image, sample_labels = extract_images_and_labels(sample)\n",
    "        images.append(sample_image)\n",
    "        labels.append(sample_labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3eb206-f14b-47dd-a9a2-0fc9e31f9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply the functions for the training, validation and test split\n",
    "# and check how many samples we have for each split.\n",
    "\n",
    "train_images, train_labels = prepare_split(\"train\")\n",
    "print(\"We have\", len(train_images), \"training samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15bad4-032a-4f73-941f-61879242216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images, val_labels = prepare_split(\"val\")\n",
    "print(\"We have\", len(val_images), \"validation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdfacc-cd56-4456-a342-a1702a4d04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = prepare_split(\"test\")\n",
    "print(\"We have\", len(test_images), \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017d3f4-7a86-4705-ac78-8b72ac248079",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "# # Add the image data: the marker channel (red channel) and nucleus channel (blue channel).\n",
    "viewer.add_image(train_images[1], colormap=\"green\", blending=\"additive\")\n",
    "viewer.add_labels(train_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d0a56-3056-4189-8f77-9362f9298dc5",
   "metadata": {},
   "source": [
    "### 2. Build dataset, dataloader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76abf4-07f0-4533-a966-3ea207764158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyTorch functionality we need.\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Additional imports for evaluation and image transformations.\n",
    "from sklearn import metrics\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59195d-1836-4cee-b98c-f9d4f8ae9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first check if we have access to a GPU.\n",
    "# The model training will be much faster if we can use a GPU.\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. The training will be very slow!\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e41e5f-4023-4e5b-a7d1-2ab21bdbc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To provide the data for training we need to create a PyTorch Dataset.\n",
    "# Datasets provide a single example (= small image + label) for training.\n",
    "# They can also be used to process the data further. Here, we resize the images\n",
    "# to the common shape within the dataset.\n",
    "\n",
    "# You can find more information on datasets here:\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "# To create a dataset for our task we create a class that inherits\n",
    "# from the PyTorch dataset.\n",
    "class CustomDataset(Dataset):\n",
    "    # Here we define the data for creating the dataset:\n",
    "    # The small images for this dataset, the labels and the size for reshaping the images.\n",
    "    def __init__(self, images, labels, crop_size, transform=None, mask_transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.crop_size = crop_size\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    # A dataset needs a __len__ method that returns how many samples are in the dataset.\n",
    "    # Here, the number of samples corresponds to the number of small images.\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    # The __getitem__ method returns the image data and labels for a given sample index.\n",
    "    def __getitem__(self, index):\n",
    "        # Load the image data and label for this sample index.\n",
    "        image = self.images[index]\n",
    "        mask = self.labels[index]\n",
    "\n",
    "        # crop the images, so that we can fit them into memory\n",
    "        crop_shape = self.crop_size\n",
    "        shape = image.shape\n",
    "        if shape != crop_shape:\n",
    "            assert image.ndim == mask.ndim == 2\n",
    "            # bring it to common shape\n",
    "            image = resize(image, shape, preserve_range=True)\n",
    "            mask = resize(mask, shape, preserve_range=True)\n",
    "            # Random crop same excerpt from image and mask\n",
    "            i, j, h, w = v2.RandomCrop.get_params(\n",
    "                torch.tensor(image), output_size=self.crop_size\n",
    "            )\n",
    "            image = v2.functional.crop(torch.tensor(image), i, j, h, w)\n",
    "            mask = v2.functional.crop(torch.tensor(mask), i, j, h, w)\n",
    "\n",
    "        # NOTE: transform would usually be applied to the image and mask so that we can\n",
    "        # use data augmentations that act both on the image and the mask (e.g. rotations and flips)\n",
    "        # apply transform if available\n",
    "        if self.transform:\n",
    "            image = self.transform(image)           \n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)   \n",
    "            \n",
    "        # make sure to add the channel dimension to the image\n",
    "        image, mask = np.array(image), np.array(mask)\n",
    "        if image.ndim == 2:\n",
    "            image = image[None]\n",
    "\n",
    "        image = torch.tensor(image)\n",
    "        mask = torch.tensor(mask)\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8cbce6-8903-43c6-8c9b-ca38f4e4c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for one epoch\n",
    "def train_epoch(model, loader, loss, metric, optimizer):\n",
    "    model.train()\n",
    "    metric_list, loss_list = [], []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss_value = loss(pred, y)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss_value.item())\n",
    "        if metric is not None:\n",
    "            metric_value = metric(pred, y)\n",
    "            metric_list.append(metric_value.item())\n",
    "\n",
    "    if metric is not None:\n",
    "        return np.mean(loss_list), np.mean(metric_list)\n",
    "    else:\n",
    "        return np.mean(loss_list), None\n",
    "\n",
    "\n",
    "# validate the model\n",
    "def validate(model, loader, loss, metric):\n",
    "    model.eval()\n",
    "    metric_list, loss_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss_value = loss(pred, y).item()\n",
    "            loss_list.append(loss_value)\n",
    "            if metric is not None:\n",
    "                metric_value = metric(pred, y).item()\n",
    "                metric_list.append(metric_value)\n",
    "\n",
    "    if metric is not None:\n",
    "        return np.mean(loss_list), np.mean(metric_list)\n",
    "    else:\n",
    "        return np.mean(loss_list), None\n",
    "\n",
    "\n",
    "# run the whole training\n",
    "def run_training(\n",
    "    model, train_loader, val_loader, loss, metric, optimizer, n_epochs\n",
    "):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    for epoch in tqdm.trange(n_epochs):\n",
    "        epoch_train_loss, epoch_train_acc = train_epoch(model, train_loader, loss, metric, optimizer)\n",
    "        epoch_val_loss, epoch_val_acc = validate(model, val_loader, loss, metric)\n",
    "        \n",
    "        # save the loss and accuracy for plotting\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "    \n",
    "    return train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "\n",
    "# plot the metrics\n",
    "def plot(title, label, train_results, val_results, yscale='linear', save_path=None, \n",
    "         extra_pt=None, extra_pt_label=None):\n",
    "    epoch_array = np.arange(len(train_results)) + 1\n",
    "    train_label, val_label = \"Training \"+label.lower(), \"Validation \"+label.lower()\n",
    "    \n",
    "    #sns.set(style='ticks')\n",
    "\n",
    "    plt.plot(epoch_array, train_results, epoch_array, val_results, linestyle='dashed', marker='o')\n",
    "    legend = ['Train results', 'Validation results']\n",
    "    \n",
    "    if extra_pt:\n",
    "        plt.plot(*extra_pt, 'ok')\n",
    "    if extra_pt_label:\n",
    "        legend.append(extra_pt_label)\n",
    "        \n",
    "    plt.legend(legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.yscale(yscale)\n",
    "    plt.title(title)\n",
    "    \n",
    "    #sns.despine(trim=True, offset=5)\n",
    "    plt.title(title, fontsize=15)\n",
    "    if save_path:\n",
    "        plt.savefig(str(save_path), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d6ff1-bb0a-4e54-9099-7dd854e9847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to generate a random color map for a label image\n",
    "def get_random_colors(labels):\n",
    "    n_labels = len(np.unique(labels)) - 1\n",
    "    cmap = [[0, 0, 0]] + np.random.rand(n_labels, 3).tolist()\n",
    "    cmap = colors.ListedColormap(cmap)\n",
    "    return cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa5132-a477-4932-be27-a595241e156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, last_activation=None):\n",
    "        super().__init__()\n",
    "        in_features = [in_channels, 64, 128, 256]\n",
    "        out_features = [64, 128, 256, 512]\n",
    "        self.encoders = nn.ModuleList([\n",
    "            self.encoder_block(in_feats, out_feats, pool=level>0)\n",
    "            for level, (in_feats, out_feats) in enumerate(zip(in_features, out_features))\n",
    "        ])\n",
    "        self.base = self.encoder_block(512, 1024, pool=True)\n",
    "        self.last_activation = last_activation\n",
    "        \n",
    "        in_features = [1024, 512, 256, 128]\n",
    "        out_features = [512, 256, 128, 64]\n",
    "        self.decoders = nn.ModuleList([\n",
    "            self.decoder_block(in_feats, out_feats)\n",
    "            for in_feats, out_feats in zip(in_features, out_features)\n",
    "        ])\n",
    "        self.upsamplers = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(in_feats, out_feats, 2, stride=2)\n",
    "            for in_feats, out_feats in zip(in_features, out_features)\n",
    "        ])\n",
    "        self.out_conv = nn.Conv2d(out_features[-1], out_channels, 1)\n",
    "        \n",
    "    def encoder_block(self, in_feats, out_feats, pool):\n",
    "        layers = [nn.MaxPool2d(2)] if pool else []\n",
    "        layers.extend([\n",
    "            nn.Conv2d(in_feats, out_feats, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_feats, out_feats, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def decoder_block(self, in_feats, out_feats):\n",
    "        return nn.Sequential(*[\n",
    "            nn.Conv2d(in_feats, out_feats, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_feats, out_feats, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        from_encoder = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            from_encoder.append(x)\n",
    "        x = self.base(x)\n",
    "        from_encoder = from_encoder[::-1]\n",
    "        for decoder, upsampler, from_enc in zip(self.decoders, self.upsamplers, from_encoder):\n",
    "            x = decoder(torch.cat([\n",
    "                from_enc, upsampler(x)\n",
    "            ], dim=1))\n",
    "        x = self.out_conv(x)\n",
    "        if self.last_activation is not None:\n",
    "            x = self.last_activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079b0a5-1535-4f22-8bb7-bf922a512542",
   "metadata": {},
   "source": [
    "# Train with boundary channel\n",
    "To avoid merges of touching cells, we will now add a boundary channel to the learning objective, and then use it for object separation in the instance segmentation funtion. To this end, we will train a U-Net that outputs 3 channels (background, foreground, object boundary) and use the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c596f2-6fb3-401f-881e-099576b8b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the issue should now be resolved. Let's try it out.\n",
    "from scipy.ndimage import label\n",
    "try:\n",
    "    from skimage.segmentation import find_boundaries, watershed\n",
    "except ImportError:\n",
    "    fix_scipy_imports()\n",
    "    from skimage.segmentation import find_boundaries, watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e662633-3591-4f08-b9db-16aa399595ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for backgorund, 1 for foreground (= cells) and 2 for boundary (boundary pixel between cells and background or between 2 cells)\n",
    "def label_transform(mask):\n",
    "    fg_target = (mask > 0)\n",
    "    bd_target = find_boundaries(mask)\n",
    "    target = np.zeros(mask.shape, dtype=\"int64\")\n",
    "    # target[fg_target] = 1\n",
    "    # target[bd_target] = 2\n",
    "    return bd_target[None].astype(\"float64\") # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff15a7-af68-4fb9-9da3-12797c3ac8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_shape = (512, 512)\n",
    "\n",
    "# Define your dataset with new transform\n",
    "train_dataset = CustomDataset(train_images, train_labels, crop_size=crop_shape, mask_transform=label_transform)\n",
    "val_dataset = CustomDataset(val_images, train_labels, crop_size=crop_shape, mask_transform=label_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284043d-4c73-424c-bf77-2642d4c05bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "\n",
    "# You can ignore the commented code for now.\n",
    "# image_vis, targets_vis = [], []\n",
    "for ii, (image, target) in enumerate(train_dataset):\n",
    "    viewer.add_image(image[0].numpy())\n",
    "    viewer.add_labels(target[0].numpy().astype(\"uint8\"))\n",
    "    break\n",
    "    \n",
    "    # images_vis.append(image[0].numpy().squeeze())\n",
    "    # targets_vis.append(target[0].numpy().squeeze())\n",
    "    # if i >= 4: break\n",
    "\n",
    "# for i, im in enumerate(images_vis):\n",
    "#     viewer.add_image(im, name=f\"image-{i}\")\n",
    "\n",
    "# for i, target in enumerate(targets_vis):\n",
    "#     viewer.add_labels(target.astype(\"uint8\"), name=f\"target-{i}\")\n",
    "\n",
    "# viewer.grid.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd642c-7415-45fd-aeea-7c67de73d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.grid.shape = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865538b-27bb-46c0-bbec-391d9fcc8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(viewer.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4ba13-06e3-41fd-8a4f-65e9ff669073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the new label transform and make sure it's correct\n",
    "counter = 0\n",
    "for im, target in train_dataset:\n",
    "    if counter >= 3:\n",
    "        break\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].imshow(im[0].squeeze(), cmap=\"gray\")\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].imshow(target.squeeze())\n",
    "    plt.show()\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcec54-bc6b-40ba-bfb3-77cb7097f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the new unet and loss function\n",
    "model = UNet(out_channels=1, last_activation=torch.nn.Sigmoid())\n",
    "model.to(device)\n",
    "# loss_function = nn.BCEWithLogitsLoss() # nn.CrossEntropyLoss()\n",
    "# loss_function.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894a48f-aae6-4293-99d8-13e18bb95e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_em.model import UNet2d\n",
    "model = UNet2d(in_channels=1, out_channels=1, final_activation=\"Sigmoid\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63280610-8c61-4c9e-837a-bcb3224cd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_samples(input_):\n",
    "    # Get number of channels\n",
    "    num_channels = input_.size(1)\n",
    "    # Permute the channel axis to first\n",
    "    permute_axes = list(range(input_.dim()))\n",
    "    permute_axes[0], permute_axes[1] = permute_axes[1], permute_axes[0]\n",
    "    # For input shape (say) NCHW, this should have the shape CNHW\n",
    "    permuted = input_.permute(*permute_axes).contiguous()\n",
    "    # Now flatten out all but the first axis and return\n",
    "    flattened = permuted.view(num_channels, -1)\n",
    "    return flattened\n",
    "\n",
    "\n",
    "def dice_score(input_, target, eps=1e-7):\n",
    "    assert input_.shape == target.shape, f\"{input_.shape}, {target.shape}\"\n",
    "    # Flatten input and target to have the shape (C, N),\n",
    "    # where N is the number of samples\n",
    "    input_ = flatten_samples(torch.sigmoid(input_))\n",
    "    target = flatten_samples(target)\n",
    "    # Compute numerator and denominator (by summing over samples and\n",
    "    # leaving the channels intact)\n",
    "    numerator = (input_ * target).sum(-1)\n",
    "    denominator = (input_ * input_).sum(-1) + (target * target).sum(-1)\n",
    "    channelwise_score = 2 * (numerator / denominator.clamp(min=eps))\n",
    "    # take the average score over the channels\n",
    "    score = channelwise_score.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85676062-cd07-4c2c-b673-953241a41550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def forward(self, input_, target):\n",
    "        return 1.0 - dice_score(input_, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebd377-9000-44b9-a2fd-714cc7856949",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0efc9-0ab4-4460-a412-03d9a4c97e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_losses, _, val_losses, _ = run_training(model, train_loader, val_loader, loss_function, None, optimizer, n_epochs)\n",
    "plot('Loss vs. Epoch', 'Loss', train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c2ff6-824a-4692-8a5a-910c17b10d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_images, test_labels, crop_size=crop_shape, mask_transform=label_transform)\n",
    "model.eval()\n",
    "for ii in range(4):\n",
    "    im, mask = test_dataset[5]\n",
    "    with torch.no_grad():\n",
    "        pred = model(im[None].to(device)).cpu().numpy().squeeze()\n",
    "\n",
    "        viewer = napari.Viewer()\n",
    "        viewer.add_image(im.cpu().numpy().squeeze())\n",
    "        viewer.add_image(pred)\n",
    "        break\n",
    "        \n",
    "        # pred = torch.sigmoid(pred).cpu().numpy().squeeze()\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(12,7))\n",
    "        ax[0].axis(\"off\")\n",
    "        ax[0].imshow(im[0], cmap=\"gray\")\n",
    "        ax[1].axis(\"off\")\n",
    "        ax[1].imshow(mask.squeeze(), interpolation=\"nearest\")\n",
    "        ax[2].axis(\"off\")\n",
    "        ax[2].imshow(pred, interpolation=\"nearest\")\n",
    "        # ax[3].axis(\"off\")\n",
    "        # ax[3].imshow(pred[1], interpolation=\"nearest\")\n",
    "        # ax[4].axis(\"off\")\n",
    "        # ax[4].imshow(pred[2], interpolation=\"nearest\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3361a-6480-420d-bc83-8e53a6b1b615",
   "metadata": {},
   "source": [
    "### Implement and Use: Dice Loss Function\n",
    "CrossEntropyLoss and BinaryCrossEntropyLoss do not return the desired results. We call the data \"unbalanced\" because the label we want to predict is disproportionately small in relation to the background (in particular the boundary channel). Therefore, it is beneficial to use a loss function that is robust against class imbalance:\n",
    "In this part of the exercise we will use the Dice coefficient as loss.\n",
    "\n",
    "\r\n",
    "Note that we do not need to predict a channel for background when using the dice coefficient as a loss (unlike with Cross Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6d22d-2e39-49e0-bcac-a4f9b0bcda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disproportional\n",
    "img, mask = test_dataset[0]\n",
    "num_zeros = np.count_nonzero(mask == 0)\n",
    "num_ones = np.count_nonzero(mask == 1)\n",
    "print(\"Number of zeros:\", num_zeros, \"Number of ones:\", num_ones, \"ones/zeros:\", num_ones/num_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60938927-72bc-4e20-b907-636f11e5ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_transform(mask):\n",
    "    mask = np.array(mask)\n",
    "    fg_target = (mask > 0)[None].astype(\"float32\")\n",
    "    bd_target = find_boundaries(mask)[None].astype(\"float32\")\n",
    "    return np.concatenate([fg_target, bd_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084b417-584c-4628-8573-9f1bb2b43544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset with new transform\n",
    "train_dataset = CustomDataset(train_images, train_labels, crop_size=crop_shape, mask_transform=label_transform)\n",
    "val_dataset = CustomDataset(val_images, train_labels, crop_size=crop_shape, mask_transform=label_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff939b6-a684-4f1c-b1cc-0fc85acaa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the samples from the dataset and make sure the label transform is correct\n",
    "counter = 0\n",
    "for im, target in train_dataset:\n",
    "    if counter > 3:\n",
    "        break\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(16, 16))\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].imshow(im[0], cmap=\"gray\")\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].imshow(target[0])\n",
    "    ax[2].axis(\"off\")\n",
    "    ax[2].imshow(target[1])\n",
    "    plt.show()\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9dc34-d926-4882-ae35-b091be80ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override the forward function to add a sigmoid activation\n",
    "# sigmoid activation is needed in combination with the new loss function: dice loss\n",
    "class MyUNet(UNet):\n",
    "    def forward(self, x):\n",
    "        # call parent's forward method\n",
    "        x = super().forward(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809faf2a-b983-49b8-8358-8705f0b29b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyUNet(out_channels=2)\n",
    "model_unet # UNet save trained\n",
    "model.to(device)\n",
    "loss = DiceLoss()\n",
    "loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88468c3b-87ad-4fbc-b23e-bd770a63207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training again\n",
    "n_epochs = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6) # default LR -3\n",
    "metric = dice_score\n",
    "train_losses, train_accs, val_losses, val_accs = run_training(model, train_loader, val_loader, loss, metric, optimizer, n_epochs)\n",
    "plot('Loss vs. Epoch', 'Loss', train_losses, val_losses)\n",
    "plot('Accuracy vs. Epoch', 'Accuracy', train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b6014-a9be-45e4-9ab4-08b667ca6420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check out instance segmentation for a few test images\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for im, mask in train_dataset:\n",
    "        if counter > 3:\n",
    "            break\n",
    "        # predict with the model and apply sigmoid to map the prediction to the range [0, 1]\n",
    "        pred = model(im[None].to(device))\n",
    "        pred = torch.sigmoid(pred).cpu().numpy().squeeze()\n",
    "        #pred = torch.tensor(pred).cpu().numpy().squeeze()\n",
    "        #print( pred.shape)\n",
    "        # get tbe nucleus instance segmentation by applying connected components to the binarized prediction\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(16, 16))\n",
    "        ax[0].axis(\"off\")\n",
    "        ax[0].imshow(im.squeeze(), cmap=\"gray\")\n",
    "        ax[1].axis(\"off\")\n",
    "        ax[1].imshow(mask[0], cmap=get_random_colors(mask[0]), interpolation=\"nearest\")\n",
    "        ax[2].axis(\"off\")\n",
    "        ax[2].imshow(mask[1], cmap=\"gray\")\n",
    "        ax[3].axis(\"off\")\n",
    "        ax[3].imshow(pred[0], cmap=get_random_colors(pred[0]), interpolation=\"nearest\")\n",
    "        ax[4].axis(\"off\")\n",
    "        ax[4].imshow(pred[1], cmap=get_random_colors(pred[1]), interpolation=\"nearest\")\n",
    "        print( pred[0].mean())\n",
    "        print( pred[1].mean())\n",
    "        plt.show()\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1b37b-f6a7-4881-928d-22b04c902643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro_sam",
   "language": "python",
   "name": "micro_sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
